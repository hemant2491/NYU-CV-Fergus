{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwreVPXZIzZB"
      },
      "source": [
        "# Computer Vision Assignment 2\n",
        "---\n",
        "\n",
        "Semester: **Fall 2022**\n",
        "\n",
        "Due date: **October 27th 2022, 11.59PM EST.**\n",
        "\n",
        "## Instructions\n",
        "---\n",
        "\n",
        "You should perform this assignment using Google Colab. \n",
        "* Before starting, clone this assignment using `File > Save a copy in Drive`. \n",
        "* After you're done, go through the notebook and ensure that you have answered all questions.\n",
        "* Finally, submit the ipynb `File > Download > Download .ipynb` on brightspace\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "---\n",
        "\n",
        "This is a minimal notebook showing the basics of object detection using ~300 lines of code. In this assignment you will code an object detection network with functionality similiar to a Single Shot (or YOLO) detector.\n",
        " \n",
        "For our purposes, we shall divide each image into rectangular crops called anchors. The job of our object detector is to classify each anchor as containing an object or not. If the anchor contains an object, we want to find out the change in the anchor position and size to best fit the object in it. Hence, object detection is a multi task network which does classification and regression."
      ],
      "metadata": {
        "id": "00B3WeE5GnLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load helper code\n",
        "%cd /content\n",
        "!rm -rf nyu-cv-object-detection\n",
        "!git clone https://github.com/nikhilweee/nyu-cv-object-detection\n",
        "%cd nyu-cv-object-detection"
      ],
      "metadata": {
        "id": "BQ7S-qvdpYxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ak37idDZIzZC"
      },
      "source": [
        "import torch\n",
        "import matplotlib\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from utils.plot import visualize\n",
        "from utils.encoder import ResNet18Encoder\n",
        "from utils.dataset import get_dataloaders\n",
        "from torchvision.ops import box_iou, nms\n",
        " \n",
        "trainloader, valloader = get_dataloaders(batch_size=4)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        " \n",
        "matplotlib.rcParams['figure.figsize'] = (10.0, 10.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT-JnR9WIzZG"
      },
      "source": [
        "## Dataset\n",
        "---\n",
        "\n",
        "First, let's introduce our dataset. We shall work with an artificially generated dataset of different shapes (squares, triangles and circles) spread throughout an image. We have a Pytorch dataloader which outputs a `sample` and `target` pair. The `sample` contains the image and the `target` contains information about the bounding boxes. We have visualized the dataset below. You dont need to write any code here. However, feel free to dig into the functions that are called to better understand the structure of the dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdSVb1pYIzZJ"
      },
      "source": [
        "sample, target = iter(trainloader).next()\n",
        "sample = torch.stack(sample, dim=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VztoWXFtIzZL"
      },
      "source": [
        "visualize(sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57B5bLG-IzZN"
      },
      "source": [
        "visualize(sample, target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utFxVXjqIzZQ"
      },
      "source": [
        "## Targets\n",
        "---\n",
        "\n",
        "The labels (or `target`s) for each `sample` in the datset have 2 kinds of information.\n",
        "\n",
        "1. The bounding boxes which are a tensor of size (N x 4).  \n",
        "Here is N is the number of objects in the image and 4 corresponds to:\n",
        "\n",
        "  a. top left x coordinate  \n",
        "  b. top left y coordinate  \n",
        "  c. bottom right x coordinate  \n",
        "  d. bottom right y coordinate  \n",
        "\n",
        "2. The classification labels which are a tensor of size (N x 1).  \n",
        "Here N corresponds to the number of objects in the image.  \n",
        "The 1 corresponds to the label of the object (whether it is a triangle, square or circle).\n",
        "\n",
        "For your convenience, here's what a `target` looks like.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHGsaaHMIzZQ",
        "tags": []
      },
      "source": [
        "print(target[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq87yw2bIzZT"
      },
      "source": [
        "## TODO 1 [20%]: Anchors\n",
        "---\n",
        "\n",
        "As described before, our object detection network heavily builds upon the concept of anchors. Each image is divided into equally spaced rectangles of different sizes. These rectangles are called anchors. Anchors are best explained through this image.\n",
        "\n",
        "![Anchor Boxes](https://lilianweng.github.io/lil-log/assets/images/SSD-framework.png)\n",
        "\n",
        "Our synthetic dataset has images of size 128x128. We shall divide the image into anchors of three different sizes - 40x40, 50x50 and 60x60. We shall use a stride length of 16, which is to say that two anchors of the same size will be separated by 16 pixels (horizontally and vertically). Keeping this in mind, we shall have 64 anchors (128/16 = 8 anchors horizontally and 128/16 = 8 anchors vertically) corresponding to each size. Since we are working with three different sizes, the image will be divided into 64x3 = 192 anchors.\n",
        "\n",
        "Your first job is to write the code that creates these 192 anchors from an image of size 128x128. Each anchor will be represented by a 4-tuple (top left x, top left y, bottom right x, bottom right y). Therefore, the output of the function below should be a tensor of size `[192,4]`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvcyW6AkIzZZ",
        "tags": []
      },
      "source": [
        "def get_anchors():\n",
        "    \"\"\"\n",
        "    Generate 192 boxes where each box is represented by :\n",
        "    [top_left_x, top_left_y, bottom_right_x, bottom_right_y]\n",
        "\n",
        "    Each anchor position should generate 3 boxes according to the scales and ratios given.\n",
        "\n",
        "    Return this result as a numpy array of size [192,4]\n",
        "    \"\"\"\n",
        "    stride = 16\n",
        "    image_size = 128\n",
        "    scales = [40, 50, 60]\n",
        "    \n",
        "    anchors = []\n",
        "\n",
        "    # TODO: Generate anchors\n",
        "\n",
        "    return anchors\n",
        "\n",
        "anchors = get_anchors()\n",
        "\n",
        "assert anchors.size() == (192,4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRmSTATwIzZZ"
      },
      "source": [
        "## TODO 2 [10%]: Model\n",
        "---\n",
        "\n",
        "Our object detection model will consist of two 'heads'. Both heads will process each of the 192 anchors that you generated in the previous section. The two heads are:\n",
        "\n",
        "1. A **classification head** (`cls` layer in the picture) to detect whether the anchor has an object in it or not. This is done by measuring the IOU of the anchor with each of the bounding boxes from the `target`. If the IOU is > 0.7, we say that the chunk contains the object. If it is less than 0.3, we say the chunk only contains the background. If the value is somewhere in between, we say that it is a bad chunk and do not use it towards loss computation.\n",
        "\n",
        "2. A **regression head** (`reg` layer in the picture) to calculate the change in height, width and the centre coordinates of the anchor to best fit the object. These offsets are 4 values for the height, width, center x and y coordinates.\n",
        "\n",
        "A pictorial representation of the model is shown below. However, instead of `2k` in the given image we just have k scores for the `cls` layer. This is because we group triangles, squares and circles into one foreground class for this assignment. Of course, we can have seperate predictions for each class but that is added complexity and is not implemented here.\n",
        "\n",
        "![Model Architecture](https://cdn-images-1.medium.com/max/1600/1*7heX-no7cdqllky-GwGBfQ.png)\n",
        "\n",
        "\n",
        "Your next job is to construct the model. Most of it is already implemented, you just have to add the classification and regression heads."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yt--F8HcIzZU"
      },
      "source": [
        "class ShapesModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # for each grid in the feature map we have 3 anchors of sizes: 40x40, 50x50, 60x60\n",
        "        num_anchors = 3\n",
        "\n",
        "        # regular resnet 18 encoder\n",
        "        self.encoder = ResNet18Encoder()\n",
        "\n",
        "        # a small conv net\n",
        "        self.conv = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # TODO: Add a convolutional Layer to predict the class. This is a head that predicts whether an anchor contains an object or not.\n",
        "        self.cls = None\n",
        "\n",
        "        # TODO: Add a convolutional Layer to predict the offsets. This is a regression head that calculates 4 offsets for each anchor.\n",
        "        self.reg = None\n",
        "\n",
        "\n",
        "    def permute_and_flatten(self, layer, N, A, C, H, W):\n",
        "        # helper function that rearranges the input for the loss function\n",
        "\n",
        "        layer = layer.view(N, -1, C, H, W)\n",
        "        layer = layer.permute(0, 3, 4, 1, 2)\n",
        "        layer = layer.reshape(N, -1, C)\n",
        "\n",
        "        return layer\n",
        "\n",
        "\n",
        "    def post_process(self, cls_pred, reg_pred):\n",
        "        # helper function that gets outputs in the right shape for applying the loss\n",
        "\n",
        "        N, AxC, H, W = cls_pred.size()\n",
        "        Ax4 = reg_pred.size(1)\n",
        "\n",
        "        A = Ax4 // 4\n",
        "        C = AxC // A\n",
        "\n",
        "        cls_pred = self.permute_and_flatten(\n",
        "            cls_pred, N, A, C, H, W\n",
        "        )\n",
        "\n",
        "        reg_pred = self.permute_and_flatten(\n",
        "            reg_pred, N, A, 4, H, W\n",
        "        )\n",
        "\n",
        "        return cls_pred.squeeze(), reg_pred.squeeze()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.encoder(x)\n",
        "\n",
        "        # we take the 3rd output feature map of \n",
        "        # size 8 x 8 from the resnet18 encoder \n",
        "\n",
        "        x = x[3]\n",
        "\n",
        "        x = F.relu(self.conv(x))\n",
        "\n",
        "        cls_pred = self.cls(x)\n",
        "        reg_pred = self.reg(x)\n",
        "\n",
        "        cls_pred, reg_pred = self.post_process(cls_pred, reg_pred)\n",
        "\n",
        "        return cls_pred, reg_pred\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-tnPWV9IzZX",
        "tags": []
      },
      "source": [
        "model = ShapesModel()\n",
        "sample, target = iter(trainloader).next()\n",
        "sample = torch.stack(sample, dim=0)\n",
        "cls_pred, reg_pred = model(sample)\n",
        "\n",
        "assert cls_pred.size() == (4, 192) and reg_pred.size() == (4, 192, 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNbDq_8xIzZb"
      },
      "source": [
        "## TODO 3 [15%]: Preprocess\n",
        "---\n",
        "\n",
        "Now that we know how our model works, we need to prepare our dataset so that it can be used to train our model. Recall that our dataset provides bounding boxes and labels for each image. However, our model expects a class label and regression offsets for each anchor. In the following cells, you will implement `get_labels_and_offsets`, which calculates class labels and offsets for each anchor.\n",
        "\n",
        "Offsets are calculated in case the anchor has an IOU > 0.7 with any real object. The offsets are calculated using `compute_offsets`, which has already been provided to you. You may also need to use [`torchvision.ops.box_iou`](https://pytorch.org/vision/main/generated/torchvision.ops.box_iou.html)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_offsets(targets, anchors):\n",
        "    \"\"\"\n",
        "    This function returns the offsets that need to applied to anchors to morph them into targets.\n",
        "    Both anchors and targets should be of the same shape, N x 4.\n",
        "    The output would be offsets given in a torch tensor of size N x 4.\n",
        "    \"\"\"\n",
        "    t_width = targets[:, 2] - targets[:, 0]\n",
        "    t_height = targets[:, 3] - targets[:, 1]\n",
        "    t_center_x = targets[:, 0] + 0.5 * t_width\n",
        "    t_center_y = targets[:, 1] + 0.5 * t_height\n",
        "\n",
        "    a_width = anchors[:, 2] - anchors[:, 0]\n",
        "    a_height = anchors[:, 3] - anchors[:, 1]\n",
        "    a_center_x = anchors[:, 0] + 0.5 * a_width\n",
        "    a_center_y = anchors[:, 1] + 0.5 * a_height\n",
        "\n",
        "    delta_x = (a_center_x - t_center_x) / t_width\n",
        "    delta_y = (a_center_y - t_center_y) / t_height\n",
        "    delta_scale_x = torch.log(a_width / t_width)\n",
        "    delta_scale_y = torch.log(a_height / t_height)\n",
        "\n",
        "    offsets = torch.cat(\n",
        "        [\n",
        "            delta_x.unsqueeze(0),\n",
        "            delta_y.unsqueeze(0),\n",
        "            delta_scale_x.unsqueeze(0),\n",
        "            delta_scale_y.unsqueeze(0),\n",
        "        ],\n",
        "        dim=0,\n",
        "    )\n",
        "    return offsets.permute(1, 0)"
      ],
      "metadata": {
        "id": "I8wTThl0hebC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMRwLlVqIzZc"
      },
      "source": [
        "def get_labels_and_offsets(targets, anchors):\n",
        "    '''\n",
        "    INPUT:\n",
        "    targets: [N x 4]: Bounding boxes in the image.\n",
        "    anchors: [192 x 4]: Anchor boxes of an image. \n",
        "\n",
        "    OUTPUT: \n",
        "    labels: [192 x 1]: Class labels for each anchor. 1 is for foreground, 0 is for background and -1 is for a bad anchor.\n",
        "    offsets: [192 x 4]: Offsets for anchor to best fit the bounding box object. Calculated if class label is 1.\n",
        "\n",
        "    '''\n",
        "    high_threshold = 0.7\n",
        "    low_threshold = 0.3\n",
        "\n",
        "    N = anchors.size(0)\n",
        "    \n",
        "    # TODO: Calculate labels and offsets\n",
        "\n",
        "    return labels.squeeze(), offsets.squeeze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geYdBXveIzZe"
      },
      "source": [
        "def get_batch(sample, target):\n",
        "    '''\n",
        "    Input\n",
        "    target => Set of bounding boxes for each image.\n",
        "    Sample => Each image\n",
        "    Output:\n",
        "    Bounding box offsets and class labels for each anchor.\n",
        "    '''\n",
        "\n",
        "    all_labels = []\n",
        "    all_offsets = []\n",
        "    for s, t in zip(sample, target):\n",
        "        targets = t['bounding_box'].to(device)\n",
        "        labels, offsets = get_labels_and_offsets(targets, anchors)\n",
        "        all_labels.append(labels)\n",
        "        all_offsets.append(offsets)\n",
        "    \n",
        "    all_labels = torch.stack(all_labels, dim=0)\n",
        "    all_offsets = torch.stack(all_offsets, dim=0)\n",
        "\n",
        "    return all_labels, all_offsets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MaTWpH2IzZg",
        "tags": []
      },
      "source": [
        "sample, target = iter(trainloader).next()\n",
        "sample = torch.stack(sample, dim=0)\n",
        "\n",
        "labels, offsets = get_batch(sample, target)\n",
        "assert labels.size() == (4, 192) and offsets.size() == (4, 192, 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5S_E5BmIzZj"
      },
      "source": [
        "This is the meat of object detection right there ! A major part of the exercise is the correct calculation of the anchors for an image and its ground truth. Now that this is done, we can move on to our neural network training code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlcVJzGlIzZj"
      },
      "source": [
        "## TODO 4 [20%]: Loss\n",
        "---\n",
        "\n",
        "The two heads in our model will use different losses.\n",
        "\n",
        "1. The classification head shall use the binary cross entropy loss. The loss is calculated only for those anchors which have been classified as foreground or background (IOU > 0.7 or IOU < 0.3) and should not be calculated for anchors with (0.3 < IOU < 0.7).\n",
        "\n",
        "Since there are too many background anchors, the model can easily predict `background` for each anchor and still get a high accuracy. To prevent this, we may sample the foreground more frequently (say 3x more than the background). This strategy of sampling less frequent examples more often is called negative sampling. Since we're dealing with relatively small number of anchors (192), this problem shouldn't affect us yet. However, if we were to use more anchors, we would have to be more careful.\n",
        "\n",
        "2. For the regression head, use a Smooth L1 loss. This works well and prevents outliers. Only calculate this loss when there is a valid offset.\n",
        "\n",
        "Your next task is to write the code for the two losses. \n",
        "\n",
        "**Extra Credit**: Implement negative sampling as described above.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi13XEtKI6pj"
      },
      "source": [
        "def label_loss(pred_label, gt_label):\n",
        "    # TODO return label loss\n",
        "\n",
        "def offset_loss(pred_offset, gt_offset, gt_label):\n",
        "    # TODO return offset loss \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u42yvSLCI6pl"
      },
      "source": [
        "## TODO 5 [10%]: Training \n",
        "---\n",
        "\n",
        "This is the training function. It is complete in itself, but you're free to change this code to suit your needs. Make sure that your model is able to train well without any hiccups. In particular, we're looking for decreasing loss values without any NaNs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trximgTSIzZj"
      },
      "source": [
        "def train(epoch, model, trainloader, optimizer):\n",
        "        total_loss = 0\n",
        "        running_offset_loss = 0\n",
        "        running_label_loss = 0\n",
        "        for i, (images, targets) in enumerate(trainloader):\n",
        "            images = torch.stack(images, dim=0).to(device)\n",
        "            gt_labels, gt_offsets = get_batch(images, targets)\n",
        "            pred_labels, pred_offsets = model(images)\n",
        "            loss_label = label_loss(pred_labels, gt_labels)\n",
        "            loss_offset = offset_loss(pred_offsets, gt_offsets, gt_labels)\n",
        "            loss = loss_label + loss_offset\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_label_loss += loss_label.item()\n",
        "            running_offset_loss += loss_offset.item()\n",
        "\n",
        "            if (i+1) % 500 == 0:\n",
        "                print(f'Iter {i+1} | Class Loss: {running_label_loss / (i+1)} Offset Loss: {running_offset_loss / (i+1)}')\n",
        "            \n",
        "        avg_running_label_loss = float(running_label_loss / len(trainloader))\n",
        "        avg_running_offset_loss = float(running_offset_loss / len(trainloader))\n",
        "        print(f'Epoch: {epoch+1} | Class Loss: {avg_running_label_loss}, Offset Loss: {avg_running_offset_loss} \\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8f5IswpIzZl"
      },
      "source": [
        "# TODO: Train your model\n",
        "\n",
        "model = ShapesModel()\n",
        "model.to(device)\n",
        "optimizer = torch.optim.SGD(\n",
        "    model.parameters(),\n",
        "    lr=0.1, momentum=0.9, weight_decay=1e-4\n",
        ")\n",
        "for epoch in range(5):\n",
        "    train(epoch, model, trainloader, optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqX8k4qYIzZn"
      },
      "source": [
        "## Validation\n",
        "---\n",
        "\n",
        "Now that we have a trained model, let's move on to visualizing some results. First, let's have a look at the validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRWPArY7IzZo"
      },
      "source": [
        "sample, target = iter(valloader).next()\n",
        "sample = torch.stack(sample,dim=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXpoJRadIzZq"
      },
      "source": [
        "visualize(sample, target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAZPrnKcl6oU"
      },
      "source": [
        "\n",
        "## Non-Maximal Supression\n",
        "---\n",
        "To visualize predictions from our model, we perform NMS on the final predictions to clear up the output. This is done as a lot of anchors near to the actual object will fire resulting in a lot of boxes. NMS handles these multiple predictions by merging and giving a single box for a predicted object instead of multiple. This is best shown through the following image.\n",
        "\n",
        "![NMS](https://miro.medium.com/max/1000/0*WI5_K3bAbYaRyzE-.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO 6 [25%]: Visualize\n",
        "---\n",
        "\n",
        "Now onto the final stretch! Your final task is to write code to visualize the model predictions on an image from the validation set. \n",
        "\n",
        "\n",
        "You will implement the `visualize_preds` function. This function will perform three subtasks.  \n",
        "  (a). Select what anchors to draw on the image. Generally, if the foreground probability is > 0.7, it's a good idea to use the anchor.  \n",
        "  (b). Adjust the anchors using the offsets. For this, you will need to _invert_ the `compute_offsets` function described above.  \n",
        "Specifically, you will write the function `apply_offsets` that is related to `compute_offsets` in the following manner.\n",
        "```\n",
        "targets = apply_offsets(anchors, compute_offsets(targets, anchors))\n",
        "```\n",
        "  (c). Finally, you will use NMS to filter out anchors. You can use [`torchvision.ops.nms`](https://pytorch.org/vision/stable/generated/torchvision.ops.nms.html)"
      ],
      "metadata": {
        "id": "Dea_FemSY2HI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_offsets(anchors, offsets):\n",
        "    \"\"\"\n",
        "    This function appiles offsets to anchors and returns the result.\n",
        "    Both anchors and offsets should be of the same shape, N x 4.\n",
        "    offsets is the result from compute_offsets function.\n",
        "    \"\"\"\n",
        "    # TODO: apply offsets to anchors\n"
      ],
      "metadata": {
        "id": "T9uJM-TXcxXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4H0o2tuIzZt"
      },
      "source": [
        "def visualize_preds(model, sample):\n",
        "    model = model.to('cpu')\n",
        "\n",
        "    label_preds, offset_preds = model(sample)\n",
        "\n",
        "    # TODO: select foreground anchors, apply offsets and use NMS\n",
        "\n",
        "    visualize(sample, predictions)\n",
        "\n",
        "visualize_preds(model, sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCNCE3PenkNa"
      },
      "source": [
        "## Conclusion\n",
        "---\n",
        "Not bad, right ? This is a very barebones implementation and trained for a very short amount of epochs. However this is the gist of most anchor based state of the art detectors out today. We can improve accuracy by doing multiscale training and incorporating a Feature Pyramid Network amongst other things, but that is for you to discover :)\n",
        "\n",
        "## References\n",
        "---\n",
        "\n",
        "Some foundational/good papers on object detection. This list is non exhaustive\n",
        "\n",
        "1. [Single Shot Detector](https://arxiv.org/abs/1512.02325)\n",
        "2. [YOLO](https://arxiv.org/abs/1612.08242)\n",
        "3. [Faster RCNN](https://arxiv.org/abs/1506.01497)\n",
        "4. [Feature Pyramid Network](https://arxiv.org/abs/1612.03144)\n",
        "5. Bonus Read: [DETR- State of the art Object detector based on Transformers](https://arxiv.org/abs/2005.12872)\n"
      ]
    }
  ]
}